{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os , glob\nimport nibabel as nib\n#os.getcwd()\n# os.chdir(r'C:\\Users\\Armelle\\feta1.2.1\\derivatives')\n# os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:45:06.835225Z","iopub.execute_input":"2022-08-03T10:45:06.835623Z","iopub.status.idle":"2022-08-03T10:45:06.840930Z","shell.execute_reply.started":"2022-08-03T10:45:06.835588Z","shell.execute_reply":"2022-08-03T10:45:06.839777Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport PIL\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.util import montage \nimport skimage.transform as skTrans\nfrom skimage.transform import rotate\nfrom skimage.transform import resize\nfrom PIL import Image, ImageOps  \n\n\n# neural imaging\nimport nilearn as nl\nimport nibabel as nib\nimport nilearn.plotting as nlplt\n!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif \nimport gif_your_nifti.core as gif2nif\n\n\n# ml libs\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n\n# Make numpy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:45:06.873653Z","iopub.execute_input":"2022-08-03T10:45:06.873930Z","iopub.status.idle":"2022-08-03T10:45:20.533742Z","shell.execute_reply.started":"2022-08-03T10:45:06.873904Z","shell.execute_reply":"2022-08-03T10:45:20.532496Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"# DEFINE seg-areas  \nSEGMENT_CLASSES = {\n    0 : 'CSF',\n    1 : 'BS', # or NON-ENHANCING tumor CORE\n    2 : 'CBM',\n    3 : 'GM' ,# original 4 -> converted into 3 later\n    4 : 'SGM',\n    5 : 'LV',\n    6 : 'WM',\n#     7 : 'CBM',\n}\n\n# there are 155 slices per volume\n# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \nVOLUME_SLICES = 256 \nVOLUME_START_AT = 0 # first slice of volume that we will include","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:45:20.536154Z","iopub.execute_input":"2022-08-03T10:45:20.536849Z","iopub.status.idle":"2022-08-03T10:45:20.544938Z","shell.execute_reply.started":"2022-08-03T10:45:20.536801Z","shell.execute_reply":"2022-08-03T10:45:20.543828Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/fetabraindataset/FeTA_1.2.1_BIDS/feta1.2.1/derivatives/sub-feta001/anat/'\n\ntest_image_irm=nib.load(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR.nii').get_fdata()\ntest_image_bs=nib.load(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR_label-BS_mask.nii').get_fdata()\ntest_image_csf=nib.load(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR_label-CSF_mask.nii').get_fdata()\n# test_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1ce.nii').get_fdata()\n# test_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t2.nii').get_fdata()\ntest_mask=nib.load(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR_label-CSF_mask.nii').get_fdata()\n\n\nfig, (ax1,ax2,ax3,ax5) = plt.subplots(1,4, figsize = (20, 10))\nslice_w = 25\nax1.imshow(test_image_irm[:,:,test_image_irm.shape[1]//2-slice_w], cmap = 'gray')\nax1.set_title('Image IRM')\nax2.imshow(test_image_bs[:,:,128], cmap = 'gray')\nax2.set_title('mask bs')\nax3.imshow(test_image_csf[:,:,test_image_csf.shape[0]//2-slice_w], cmap = 'gray')\nax3.set_title('mask csf')\n# ax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')\n# ax4.set_title('Image t2')\n# ax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])\n# ax5.set_title('Mask')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:45:20.547104Z","iopub.execute_input":"2022-08-03T10:45:20.547936Z","iopub.status.idle":"2022-08-03T10:45:21.224342Z","shell.execute_reply.started":"2022-08-03T10:45:20.547889Z","shell.execute_reply":"2022-08-03T10:45:21.223419Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"# Skip 50:-50 slices since there is not much to see\nfig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_image_bs[50:-50,:,:]), 90, resize=True), cmap ='gray')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:45:21.226886Z","iopub.execute_input":"2022-08-03T10:45:21.227960Z","iopub.status.idle":"2022-08-03T10:45:23.431851Z","shell.execute_reply.started":"2022-08-03T10:45:21.227922Z","shell.execute_reply":"2022-08-03T10:45:23.430140Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"# Skip 50:-50 slices since there is not much to see\nfig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-08-03T10:45:23.437073Z","iopub.execute_input":"2022-08-03T10:45:23.440088Z","iopub.status.idle":"2022-08-03T10:45:25.481793Z","shell.execute_reply.started":"2022-08-03T10:45:23.440050Z","shell.execute_reply":"2022-08-03T10:45:25.480837Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"# shutil.copy2(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR.nii', './test_gif_BraTS20_Training_001_flair.nii')\n# gif2nif.write_gif_normal('../input/fetabraindataset/FeTA_1.2.1_BIDS/feta1.2.1/derivatives/sub-feta001/anat/sub-feta001_T2w-SR.nii')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-08-03T10:45:25.483077Z","iopub.execute_input":"2022-08-03T10:45:25.484733Z","iopub.status.idle":"2022-08-03T10:45:25.489527Z","shell.execute_reply.started":"2022-08-03T10:45:25.484692Z","shell.execute_reply":"2022-08-03T10:45:25.488172Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"markdown","source":"![](http://)","metadata":{"editable":false}},{"cell_type":"code","source":"niimg = nl.image.load_img(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR.nii')\nnimask = nl.image.load_img(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR_label-BS_mask.nii')\nnimask2 = nl.image.load_img(TRAIN_DATASET_PATH + 'sub-feta001_T2w-SR_label-CSF_mask.nii')\n\nfig, axes = plt.subplots(nrows=5, figsize=(30, 40))\n\n\nnlplt.plot_anat(niimg,\n                title='sub-feta001_T2w-SR.niii plot_anat',\n                axes=axes[0])\n\nnlplt.plot_epi(niimg,\n               title='sub-feta001_T2w-SR.nii plot_epi',\n               axes=axes[1])\n\nnlplt.plot_img(niimg,\n               title='sub-feta001_T2w-SR.nii plot_img',\n               axes=axes[2])\n\nnlplt.plot_roi(nimask, \n               title='sub-feta001_T2w-SR.nii with mask plot_roi',\n               bg_img=niimg, \n               axes=axes[3], cmap='Paired')\nnlplt.plot_roi(nimask2, \n               title='sub-feta001_T2w-SR.nii with mask plot_roi',\n               bg_img=niimg, \n               axes=axes[4], cmap='Paired')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:45:25.491180Z","iopub.execute_input":"2022-08-03T10:45:25.491540Z","iopub.status.idle":"2022-08-03T10:47:03.259572Z","shell.execute_reply.started":"2022-08-03T10:45:25.491503Z","shell.execute_reply":"2022-08-03T10:47:03.258649Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"code","source":"# dice loss as defined above for 4 classes\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    class_num = 4\n    for i in range(class_num):\n        y_true_f = K.flatten(y_true[:,:,:,i])\n        y_pred_f = K.flatten(y_pred[:,:,:,i])\n        intersection = K.sum(y_true_f * y_pred_f)\n        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss / class_num\n#    K.print_tensor(total_loss, message=' total dice coef: ')\n    return total_loss\n\n\n\n\n\n\n# Computing Precision \ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    \n# Computing Sensitivity      \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\n\n# Computing Specificity\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:47:03.261131Z","iopub.execute_input":"2022-08-03T10:47:03.261699Z","iopub.status.idle":"2022-08-03T10:47:03.275952Z","shell.execute_reply.started":"2022-08-03T10:47:03.261664Z","shell.execute_reply":"2022-08-03T10:47:03.274846Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE=128","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-08-03T10:47:03.277667Z","iopub.execute_input":"2022-08-03T10:47:03.278403Z","iopub.status.idle":"2022-08-03T10:47:03.289410Z","shell.execute_reply.started":"2022-08-03T10:47:03.278357Z","shell.execute_reply":"2022-08-03T10:47:03.288131Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"# IMG_SIZE=256","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:47:03.294093Z","iopub.execute_input":"2022-08-03T10:47:03.294525Z","iopub.status.idle":"2022-08-03T10:47:03.299541Z","shell.execute_reply.started":"2022-08-03T10:47:03.294450Z","shell.execute_reply":"2022-08-03T10:47:03.298531Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"code","source":"# source https://naomi-fridman.medium.com/multi-class-image-segmentation-a5cc671e647a\n\ndef build_unet(inputs, ker_init, dropout):\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n    \n    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n    \n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n    \n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n    \n    \n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n    drop5 = Dropout(dropout)(conv5)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n    \n    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n    merge = concatenate([conv1,up], axis = 3)\n    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n    \n    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n    \n    return Model(inputs = inputs, outputs = conv10)\n\ninput_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n\nmodel = build_unet(input_layer, 'he_normal', 0.2)\n# model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy'] )\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001), metrics = ['accuracy', dice_coef, precision, sensitivity, specificity] )","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:15:55.443568Z","iopub.execute_input":"2022-08-03T11:15:55.444076Z","iopub.status.idle":"2022-08-03T11:15:55.662631Z","shell.execute_reply.started":"2022-08-03T11:15:55.444043Z","shell.execute_reply":"2022-08-03T11:15:55.661673Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"# plot_model(model, \n#            show_shapes = True,\n#            show_dtype=False,\n#            show_layer_names = True, \n#            rankdir = 'TB', \n#            expand_nested = False, \n#            dpi = 70)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-08-03T10:47:03.526627Z","iopub.execute_input":"2022-08-03T10:47:03.526987Z","iopub.status.idle":"2022-08-03T10:47:03.531881Z","shell.execute_reply.started":"2022-08-03T10:47:03.526954Z","shell.execute_reply":"2022-08-03T10:47:03.530690Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/fetabraindataset/FeTA_1.2.1_BIDS/feta1.2.1/derivatives'\n\n# lists of directories with studies\ntrain_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n# print train_and_val_directories[0]\n\n\n# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n# train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n\n\ndef pathListIntoIds(dirList):\n    x = []\n    \n    print(len(dirList))\n    \n    for i in range(0,len(dirList)-10):\n#         print(dirList[i][dirList[i].rfind('/')+1:])\n        x.append(dirList[i][dirList[i].rfind('/')+1:])\n    return x\n\ntrain_and_test_ids = pathListIntoIds(train_and_val_directories); \n# print(train_and_test_ids)\n\n    \ntrain_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n# print(train_test_ids)\nprint(val_ids)\nif 'sub-feta040'in val_ids:\n    val_ids.remove('sub-feta040')\nif 'sub-feta041'in val_ids: \n    val_ids.remove('sub-feta041')\nif 'sub-feta042'in val_ids:\n    val_ids.remove('sub-feta042')\nif 'sub-feta043'in val_ids:\n    val_ids.remove('sub-feta043')\nif 'sub-feta044'in val_ids:\n    val_ids.remove('sub-feta044')\nif 'sub-feta045'in val_ids:\n    val_ids.remove('sub-feta045')\nif 'sub-feta046'in val_ids:\n    val_ids.remove('sub-feta046')\nif 'sub-feta047'in val_ids:\n    val_ids.remove('sub-feta047')\nif 'sub-feta048'in val_ids:\n    val_ids.remove('sub-feta048')\nif 'sub-feta049'in val_ids:\n    val_ids.remove('sub-feta049')\nif 'sub-feta050'in val_ids:\n    val_ids.remove('sub-feta050')\nprint(val_ids)\n\n    \n    \n\n    \n    \n   \n    \ntrain_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) \nprint(test_ids)\nif 'sub-feta040'in test_ids:\n    test_ids.remove('sub-feta040')\nif 'sub-feta041'in test_ids: \n    test_ids.remove('sub-feta041')\nif 'sub-feta042'in test_ids:\n    test_ids.remove('sub-feta042')\nif 'sub-feta043'in test_ids:\n    test_ids.remove('sub-feta043')\nif 'sub-feta044'in test_ids:\n    test_ids.remove('sub-feta044')\nif 'sub-feta045'in test_ids:\n    test_ids.remove('sub-feta045')\nif 'sub-feta046'in test_ids:\n    test_ids.remove('sub-feta046')\nif 'sub-feta047'in test_ids:\n    test_ids.remove('sub-feta047')\nif 'sub-feta048'in test_ids:\n    test_ids.remove('sub-feta048')\nif 'sub-feta049'in test_ids:\n    test_ids.remove('sub-feta049')\nif 'sub-feta050'in test_ids:\n    test_ids.remove('sub-feta050')\nprint(val_ids)\n\nif 'sub-feta040'in train_ids:\n    train_ids.remove('sub-feta040')\nif 'sub-feta041'in train_ids: \n    train_ids.remove('sub-feta041')\nif 'sub-feta042'in train_ids:\n    train_ids.remove('sub-feta042')\nif 'sub-feta043'in train_ids:\n    train_ids.remove('sub-feta043')\nif 'sub-feta044'in train_ids:\n    train_ids.remove('sub-feta044')\nif 'sub-feta045'in train_ids:\n    train_ids.remove('sub-feta045')\nif 'sub-feta046'in train_ids:\n    train_ids.remove('sub-feta046')\nif 'sub-feta047'in train_ids:\n    train_ids.remove('sub-feta047')\nif 'sub-feta048'in train_ids:\n    train_ids.remove('sub-feta048')\nif 'sub-feta049'in train_ids:\n    train_ids.remove('sub-feta049')\nif 'sub-feta050'in train_ids:\n    train_ids.remove('sub-feta050')\n# if 'sub-feta016'in train_ids:\n#     train_ids.remove('sub-feta016')\nprint(train_ids)\n    \n# print(train_ids)\n# print(test_ids)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:47:03.533670Z","iopub.execute_input":"2022-08-03T10:47:03.534364Z","iopub.status.idle":"2022-08-03T10:47:03.560256Z","shell.execute_reply.started":"2022-08-03T10:47:03.534328Z","shell.execute_reply":"2022-08-03T10:47:03.559124Z"},"trusted":true},"execution_count":209,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(keras.utils.all_utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        Batch_ids = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(Batch_ids)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, Batch_ids):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n        y = np.zeros((self.batch_size*VOLUME_SLICES, 256, 256))\n        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n\n        \n        # Generate data\n        for c, i in enumerate(Batch_ids):\n            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n\n            data_path = os.path.join(f'{case_path}/anat', f'{i}_T2w-SR.nii');\n            flair = nib.load(data_path).get_fdata()    \n\n#             data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n#             ce = nib.load(data_path).get_fdata()\n            \n            data_path = os.path.join(f'{case_path}/anat', f'{i}_T2w-SR_label-BS_mask.nii');\n            seg = nib.load(data_path).get_fdata()\n        \n            for j in range(VOLUME_SLICES):\n                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n#                  X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n\n                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n                    \n        # Generate masks\n        y[y==4] = 3;\n        mask = tf.one_hot(y, 4);\n        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n        return X/np.max(X), Y\n        \ntraining_generator = DataGenerator(train_ids)\nvalid_generator = DataGenerator(val_ids)\ntest_generator = DataGenerator(test_ids)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:47:03.562100Z","iopub.execute_input":"2022-08-03T10:47:03.562460Z","iopub.status.idle":"2022-08-03T10:47:03.578611Z","shell.execute_reply.started":"2022-08-03T10:47:03.562411Z","shell.execute_reply":"2022-08-03T10:47:03.577386Z"},"trusted":true},"execution_count":210,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show number of data for each dir \ndef showDataLayout():\n    plt.bar([\"Train\",\"Valid\",\"Test\"],\n    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n    plt.legend()\n\n    plt.ylabel('Number of images')\n    plt.title('Data distribution')\n\n    plt.show()\n    \nshowDataLayout()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:47:03.580411Z","iopub.execute_input":"2022-08-03T10:47:03.581219Z","iopub.status.idle":"2022-08-03T10:47:03.763810Z","shell.execute_reply.started":"2022-08-03T10:47:03.581184Z","shell.execute_reply":"2022-08-03T10:47:03.762879Z"},"trusted":true},"execution_count":211,"outputs":[]},{"cell_type":"code","source":"csv_logger = CSVLogger('training.log', separator=',', append=False)\n\n\ncallbacks = [\n#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n#                               patience=2, verbose=1, mode='auto'),\n      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.0001, verbose=1),\n#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n#                             verbose=1, save_best_only=True, save_weights_only = True)\n        csv_logger\n    ]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:53:56.974689Z","iopub.execute_input":"2022-08-03T10:53:56.975386Z","iopub.status.idle":"2022-08-03T10:53:56.981289Z","shell.execute_reply.started":"2022-08-03T10:53:56.975349Z","shell.execute_reply":"2022-08-03T10:53:56.980235Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"# model.summary()\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-08-03T10:47:03.774410Z","iopub.execute_input":"2022-08-03T10:47:03.775845Z","iopub.status.idle":"2022-08-03T10:47:03.781465Z","shell.execute_reply.started":"2022-08-03T10:47:03.775804Z","shell.execute_reply":"2022-08-03T10:47:03.780423Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:47:03.783351Z","iopub.execute_input":"2022-08-03T10:47:03.784858Z","iopub.status.idle":"2022-08-03T10:47:03.797831Z","shell.execute_reply.started":"2022-08-03T10:47:03.784666Z","shell.execute_reply":"2022-08-03T10:47:03.796684Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\n\nhistory =  model.fit(training_generator,\n                    epochs=50,\n                    steps_per_epoch=len(train_ids)//2,\n                    callbacks= callbacks,\n                    validation_data = valid_generator\n                    )  \nhistory=model.fit(training_generator,steps_per_epoch=len(train_ids),\n                 callbacks=callbacks,verbose=1)\n# model.save(\"model_x1_1.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:16:21.759887Z","iopub.execute_input":"2022-08-03T11:16:21.760377Z","iopub.status.idle":"2022-08-03T11:31:26.938327Z","shell.execute_reply.started":"2022-08-03T11:16:21.760318Z","shell.execute_reply":"2022-08-03T11:31:26.936755Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,3,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\n# ax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\n# ax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\n# ax[3].legend()\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:32:39.355990Z","iopub.execute_input":"2022-08-03T11:32:39.356930Z","iopub.status.idle":"2022-08-03T11:32:39.788869Z","shell.execute_reply.started":"2022-08-03T11:32:39.356894Z","shell.execute_reply":"2022-08-03T11:32:39.785970Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"code","source":"def predictByPath(case_path,case):\n#     print(case_path)\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n#     y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE))\n    \n    vol_path = os.path.join(case_path, f'sub-feta{case}_T2w-SR.nii');\n#     print(case)\n    irm=nib.load(vol_path).get_fdata()\n    \n#     vol_path = os.path.join(case_path, f'sub-feta{case}_T2w-SR.nii');\n#     ce=nib.load(vol_path).get_fdata() \n    \n    vol_path = os.path.join(case_path, f'sub-feta{case}_T2w-SR_label-CSF_mask.nii');\n    mask_csf=nib.load(vol_path).get_fdata() \n    vol_path = os.path.join(case_path, f'sub-feta{case}_T2w-SR_label-BS_mask.nii');\n    mask_bs=nib.load(vol_path).get_fdata() \n\n    \n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(irm[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(mask_csf[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n#         y[j,:,:] = cv2.resize(mask_bs[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        \n  #  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\n    return model.predict(X/np.max(X), verbose=1)\n\n\ndef showPredictsById(case, start_slice =100 ):\n    path = f\"../input/fetabraindataset/FeTA_1.2.1_BIDS/feta1.2.1/derivatives/sub-feta{case}/anat/\"\n#     print(path)\n    csf = nib.load(os.path.join(path, f'sub-feta{case}_T2w-SR_label-CSF_mask.nii')).get_fdata()\n#     bs = nib.load(os.path.join(path, f'sub-feta{case}_T2w-SR_label-BS_mask.nii')).get_fdata()\n#     gm = nib.load(os.path.join(path, f'sub-feta{case}_T2w-SR_label-GM_mask.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'sub-feta{case}_T2w-SR.nii')).get_fdata()\n    print()\n    p = predictByPath(path,case)\n#     print(p)\n\n    csf = p[:,:,:,1]\n    bs  = p[:,:,:,2]\n\n    plt.figure(figsize=(50, 50))\n    f, axarr = plt.subplots(1,3, figsize = (50, 50)) \n\n    for i in range(3): # for each image, add brain background\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n    \n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image')\n    curr_csf=cv2.resize(csf[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_csf, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(csf[start_slice,:,:], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text(f'{SEGMENT_CLASSES[0]} predicted')\n    plt.show()\n    \n    \nshowPredictsById(case=test_ids[0][-3:])\nshowPredictsById(case=test_ids[1][-3:])\nshowPredictsById(case=test_ids[2][-3:])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:31:51.636013Z","iopub.execute_input":"2022-08-03T11:31:51.636378Z","iopub.status.idle":"2022-08-03T11:31:59.422804Z","shell.execute_reply.started":"2022-08-03T11:31:51.636346Z","shell.execute_reply":"2022-08-03T11:31:59.421655Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"code","source":"case = test_ids[2][-3:]\npath = f\"../input/fetabraindataset/FeTA_1.2.1_BIDS/feta1.2.1/derivatives/sub-feta{case}/anat/\"\ngt = nib.load(os.path.join(path, f'sub-feta{case}_T2w-SR_label-CSF_mask.nii')).get_fdata()\np = predictByPath(path,case)\n\n\ncsf = p[:,:,:,1]\n\n\n\ni=100 # slice at\neval_class = 0 #     0 : 'csf',  1 : 'bs',    2 : 'gm',    3 : 'wm'\n\n\n\ngt[gt != eval_class] = 1 # use only one class for per class evaluation \n\nresized_gt = cv2.resize(gt[:,:,i], (IMG_SIZE, IMG_SIZE))\n\nplt.figure()\nf, axarr = plt.subplots(1,2) \naxarr[0].imshow(resized_gt, cmap=\"gray\")\naxarr[0].title.set_text('ground truth')\naxarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\naxarr[1].title.set_text(f'predicted class: {SEGMENT_CLASSES[eval_class]}')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:32:16.474594Z","iopub.execute_input":"2022-08-03T11:32:16.475405Z","iopub.status.idle":"2022-08-03T11:32:17.560337Z","shell.execute_reply.started":"2022-08-03T11:32:16.475364Z","shell.execute_reply":"2022-08-03T11:32:17.559374Z"},"trusted":true},"execution_count":228,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy', dice_coef, precision, sensitivity, specificity] )\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(test_generator, batch_size=100, callbacks= callbacks)\nprint(\"test loss, test acc:\", results)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-08-03T10:50:35.632316Z","iopub.status.idle":"2022-08-03T10:50:35.633338Z","shell.execute_reply.started":"2022-08-03T10:50:35.633078Z","shell.execute_reply":"2022-08-03T10:50:35.633104Z"},"trusted":true},"execution_count":null,"outputs":[]}]}